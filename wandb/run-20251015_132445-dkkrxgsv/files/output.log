Map: 100%|██████████████████████| 20400/20400 [00:05<00:00, 3623.07 examples/s]
Map: 100%|████████████████████████| 5100/5100 [00:01<00:00, 3770.54 examples/s]
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
  0%|                                                 | 0/7650 [00:00<?, ?it/s]D:\Finrapt\intent-classification using bert\intentbert\Lib\site-packages\torch\utils\data\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.
  warnings.warn(warn_msg)
  1%|▉                                                                                                                                                                       | 41/7650 [01:46<5:24:37,  2.56s/it]Traceback (most recent call last):
{'loss': 2.854, 'grad_norm': 5.326963901519775, 'learning_rate': 4.994117647058824e-05, 'epoch': 0.0}
{'loss': 2.7893, 'grad_norm': 6.625367641448975, 'learning_rate': 4.987581699346405e-05, 'epoch': 0.01}
{'loss': 2.6313, 'grad_norm': 7.983456134796143, 'learning_rate': 4.981045751633987e-05, 'epoch': 0.01}
{'loss': 2.5292, 'grad_norm': 10.814603805541992, 'learning_rate': 4.974509803921569e-05, 'epoch': 0.02}
  File "D:\Finrapt\intent-classification using bert\train_bert.py", line 98, in <module>
    trainer.train()
    ~~~~~~~~~~~~~^^
  File "D:\Finrapt\intent-classification using bert\intentbert\Lib\site-packages\transformers\trainer.py", line 2325, in train
    return inner_training_loop(
        args=args,
    ...<2 lines>...
        ignore_keys_for_eval=ignore_keys_for_eval,
    )
  File "D:\Finrapt\intent-classification using bert\intentbert\Lib\site-packages\transformers\trainer.py", line 2740, in _inner_training_loop
    self.optimizer.step()
    ~~~~~~~~~~~~~~~~~~~^^
  File "D:\Finrapt\intent-classification using bert\intentbert\Lib\site-packages\accelerate\optimizer.py", line 179, in step
    self.optimizer.step(closure)
    ~~~~~~~~~~~~~~~~~~~^^^^^^^^^
  File "D:\Finrapt\intent-classification using bert\intentbert\Lib\site-packages\torch\optim\lr_scheduler.py", line 133, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
           ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^
  File "D:\Finrapt\intent-classification using bert\intentbert\Lib\site-packages\torch\optim\optimizer.py", line 516, in wrapper
    out = func(*args, **kwargs)
  File "D:\Finrapt\intent-classification using bert\intentbert\Lib\site-packages\torch\optim\optimizer.py", line 81, in _use_grad
    ret = func(*args, **kwargs)
  File "D:\Finrapt\intent-classification using bert\intentbert\Lib\site-packages\torch\optim\adam.py", line 247, in step
    adam(
    ~~~~^
        params_with_grad,
        ^^^^^^^^^^^^^^^^^
    ...<19 lines>...
        decoupled_weight_decay=group["decoupled_weight_decay"],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "D:\Finrapt\intent-classification using bert\intentbert\Lib\site-packages\torch\optim\optimizer.py", line 149, in maybe_fallback
    return func(*args, **kwargs)
  File "D:\Finrapt\intent-classification using bert\intentbert\Lib\site-packages\torch\optim\adam.py", line 949, in adam
    func(
    ~~~~^
        params,
        ^^^^^^^
    ...<17 lines>...
        decoupled_weight_decay=decoupled_weight_decay,
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "D:\Finrapt\intent-classification using bert\intentbert\Lib\site-packages\torch\optim\adam.py", line 858, in _fused_adam
    func(
    ~~~~^
        device_params,
        ^^^^^^^^^^^^^^
    ...<13 lines>...
        found_inf=device_found_inf,
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
KeyboardInterrupt
